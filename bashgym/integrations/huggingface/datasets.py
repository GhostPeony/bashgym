"""
HuggingFace Datasets Manager

Provides dataset upload and management capabilities for training data storage
on HuggingFace Hub. Pro users get 1TB storage and Data Studio access on private datasets.

Features:
- Upload training data (train.jsonl, val.jsonl) to HF Hub
- Auto-generate dataset cards with statistics
- Download datasets for local use
- List and delete datasets

Usage:
    from bashgym.integrations.huggingface import get_hf_client
    from bashgym.integrations.huggingface.datasets import HFDatasetManager, DatasetConfig

    client = get_hf_client()
    manager = HFDatasetManager(client)

    # Upload training data
    url = manager.upload_training_data(
        local_path=Path("./data"),
        repo_name="my-training-data",
        config=DatasetConfig(repo_name="my-training-data"),
    )

    # List datasets
    datasets = manager.list_datasets(prefix="bashgym")

    # Download a dataset
    manager.download_dataset("myorg/my-dataset", Path("./local_data"))
"""

import json
import logging
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any

from .client import (
    HuggingFaceClient,
    HFError,
    HF_HUB_AVAILABLE,
)

logger = logging.getLogger(__name__)


# =============================================================================
# Data Classes
# =============================================================================

@dataclass
class DatasetConfig:
    """Configuration for a HuggingFace Dataset."""

    repo_name: str
    """Name of the dataset repository."""

    private: bool = True
    """Whether the dataset should be private."""

    enable_viewer: bool = True
    """Enable Data Studio viewer (Pro feature on private datasets)."""

    def validate(self) -> List[str]:
        """Validate the configuration."""
        errors = []

        if not self.repo_name:
            errors.append("repo_name is required")

        # Validate repo name format
        if self.repo_name and "/" in self.repo_name:
            errors.append("repo_name should not include namespace (just the name)")

        return errors


# =============================================================================
# Dataset Card Template
# =============================================================================

DATASET_CARD_TEMPLATE = '''---
license: mit
task_categories:
  - text-generation
language:
  - en
tags:
  - bashgym
  - code-generation
  - agent-training
  - fine-tuning
size_categories:
  - {size_category}
---

# {title}

> Training dataset generated by Bash Gym on {date}

## Dataset Description

This dataset contains training examples for fine-tuning language models on agentic coding tasks.

### Statistics

| Split | Examples |
|-------|----------|
| Train | {train_count:,} |
| Validation | {val_count:,} |

**Source repositories:** {repos}

## Dataset Structure

### Data Format

NeMo-compatible JSONL format with messages array:

```json
{{"messages": [{{"role": "system", "content": "..."}}, {{"role": "user", "content": "..."}}, {{"role": "assistant", "content": "..."}}]}}
```

### Fields

- `messages`: Array of conversation turns
  - `role`: One of "system", "user", or "assistant"
  - `content`: The message content

## Usage

```python
from datasets import load_dataset

# Load the dataset
dataset = load_dataset("{repo_id}")

# Access splits
train_data = dataset["train"]
val_data = dataset["validation"]

# Iterate over examples
for example in train_data:
    messages = example["messages"]
    print(messages)
```

## Training

This dataset is designed for supervised fine-tuning (SFT) of language models:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer

model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-Coder-1.5B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Coder-1.5B-Instruct")

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    # ... training arguments
)

trainer.train()
```

---

*Generated by [Bash Gym](https://github.com/bashgym/bashgym) - Self-Improving Agentic Development Gym*
'''


# =============================================================================
# Dataset Manager
# =============================================================================

class HFDatasetManager:
    """
    Manages training datasets on HuggingFace Hub.

    Uploads JSONL training data with auto-generated dataset cards.
    Pro users get 1TB storage and Data Studio access on private datasets.

    Example:
        client = get_hf_client()
        manager = HFDatasetManager(client)

        # Upload training data
        url = manager.upload_training_data(
            local_path=Path("./data"),
            repo_name="my-training-data",
        )

        # List datasets
        datasets = manager.list_datasets(prefix="bashgym")

        # Download a dataset
        manager.download_dataset("myorg/my-dataset", Path("./local_data"))

        # Delete a dataset
        manager.delete_dataset("my-training-data")
    """

    def __init__(
        self,
        client: Optional[HuggingFaceClient] = None,
        token: Optional[str] = None,
        username: Optional[str] = None,
    ):
        """
        Initialize the Dataset manager.

        Args:
            client: HuggingFaceClient instance. If None, creates one with token.
            token: HF API token (used if client is None).
            username: Override username (useful for testing).
        """
        if client is not None:
            self._client = client
        else:
            self._client = HuggingFaceClient(token=token)

        self._username_override = username

    @property
    def client(self) -> HuggingFaceClient:
        """Get the HuggingFace client."""
        return self._client

    @property
    def username(self) -> str:
        """Get the username for operations."""
        if self._username_override:
            return self._username_override
        return self._client.namespace or ""

    def _count_lines(self, file_path: Path) -> int:
        """Count lines in a file."""
        if not file_path.exists():
            return 0
        with open(file_path, "r", encoding="utf-8") as f:
            return sum(1 for _ in f)

    def _get_size_category(self, total_count: int) -> str:
        """Get HuggingFace size category based on example count."""
        if total_count < 1000:
            return "n<1K"
        elif total_count < 10000:
            return "1K<n<10K"
        elif total_count < 100000:
            return "10K<n<100K"
        elif total_count < 1000000:
            return "100K<n<1M"
        else:
            return "n>1M"

    def upload_training_data(
        self,
        local_path: Path,
        repo_name: str,
        config: Optional[DatasetConfig] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Upload training data to HuggingFace Hub.

        Expects the local_path directory to contain:
        - train.jsonl: Training examples (required)
        - val.jsonl: Validation examples (optional)

        Args:
            local_path: Directory containing train.jsonl and optionally val.jsonl.
            repo_name: Name for the dataset repo (without namespace).
            config: Dataset configuration. If None, uses defaults.
            metadata: Additional metadata to include in metadata.json.

        Returns:
            URL of the created dataset.

        Raises:
            HFError: If upload fails.
            ValueError: If configuration is invalid.
            FileNotFoundError: If train.jsonl doesn't exist.
        """
        # Use default config if none provided
        if config is None:
            config = DatasetConfig(repo_name=repo_name)

        # Validate configuration
        errors = config.validate()
        if errors:
            raise ValueError(f"Invalid Dataset configuration: {'; '.join(errors)}")

        # Check for train.jsonl
        local_path = Path(local_path)
        train_file = local_path / "train.jsonl"
        val_file = local_path / "val.jsonl"

        if not train_file.exists():
            raise FileNotFoundError(f"train.jsonl not found in {local_path}")

        # Count examples
        train_count = self._count_lines(train_file)
        val_count = self._count_lines(val_file) if val_file.exists() else 0

        # Construct full repo ID
        repo_id = f"{self.username}/{repo_name}" if self.username else repo_name

        logger.info(f"Uploading dataset to: {repo_id}")
        logger.info(f"  Train examples: {train_count:,}")
        logger.info(f"  Val examples: {val_count:,}")
        logger.info(f"  Private: {config.private}")

        # Generate README
        readme_content = DATASET_CARD_TEMPLATE.format(
            title=f"Bash Gym Training Dataset: {repo_name}",
            date=datetime.now().strftime("%Y-%m-%d"),
            train_count=train_count,
            val_count=val_count,
            repos=metadata.get("repos", "N/A") if metadata else "N/A",
            repo_id=repo_id,
            size_category=self._get_size_category(train_count + val_count),
        )

        api = self._client.api
        if api is not None and HF_HUB_AVAILABLE:
            try:
                # Create the dataset repository
                api.create_repo(
                    repo_id=repo_id,
                    repo_type="dataset",
                    private=config.private,
                    exist_ok=True,
                )

                # Upload train.jsonl
                api.upload_file(
                    path_or_fileobj=str(train_file),
                    path_in_repo="train.jsonl",
                    repo_id=repo_id,
                    repo_type="dataset",
                )

                # Upload val.jsonl if it exists
                if val_file.exists():
                    api.upload_file(
                        path_or_fileobj=str(val_file),
                        path_in_repo="val.jsonl",
                        repo_id=repo_id,
                        repo_type="dataset",
                    )

                # Upload README.md
                api.upload_file(
                    path_or_fileobj=readme_content.encode("utf-8"),
                    path_in_repo="README.md",
                    repo_id=repo_id,
                    repo_type="dataset",
                )

                # Upload metadata.json if provided
                if metadata:
                    metadata_content = json.dumps(metadata, indent=2)
                    api.upload_file(
                        path_or_fileobj=metadata_content.encode("utf-8"),
                        path_in_repo="metadata.json",
                        repo_id=repo_id,
                        repo_type="dataset",
                    )

                logger.info(f"Dataset uploaded successfully: {repo_id}")
                return f"https://huggingface.co/datasets/{repo_id}"

            except Exception as e:
                logger.error(f"Failed to upload dataset: {e}")
                raise HFError(f"Failed to upload dataset: {e}")
        else:
            # Simulation mode
            logger.info(f"Simulating dataset upload for {repo_id}")
            return f"https://huggingface.co/datasets/{repo_id}"

    def download_dataset(self, repo_id: str, local_path: Path) -> None:
        """
        Download a dataset from HuggingFace Hub.

        Args:
            repo_id: Full dataset repo ID (e.g., "myorg/my-dataset").
            local_path: Local directory to download to.

        Raises:
            HFError: If download fails.
        """
        local_path = Path(local_path)
        local_path.mkdir(parents=True, exist_ok=True)

        logger.info(f"Downloading dataset: {repo_id} -> {local_path}")

        api = self._client.api
        if api is not None and HF_HUB_AVAILABLE:
            try:
                api.snapshot_download(
                    repo_id=repo_id,
                    repo_type="dataset",
                    local_dir=str(local_path),
                )
                logger.info(f"Dataset downloaded: {repo_id}")

            except Exception as e:
                logger.error(f"Failed to download dataset: {e}")
                raise HFError(f"Failed to download dataset: {e}")
        else:
            # Simulation mode
            logger.info(f"Simulating dataset download for {repo_id}")

    def list_datasets(self, prefix: str = "bashgym") -> List[str]:
        """
        List datasets matching the given prefix.

        Args:
            prefix: Search prefix for dataset names.

        Returns:
            List of dataset repo IDs.

        Raises:
            HFError: If listing fails.
        """
        logger.info(f"Listing datasets with prefix: {prefix}")

        api = self._client.api
        if api is not None and HF_HUB_AVAILABLE:
            try:
                datasets = api.list_datasets(
                    author=self.username,
                    search=prefix,
                )
                result = [d.id for d in datasets]
                logger.info(f"Found {len(result)} datasets")
                return result

            except Exception as e:
                logger.error(f"Failed to list datasets: {e}")
                raise HFError(f"Failed to list datasets: {e}")
        else:
            # Simulation mode
            logger.info("Simulating dataset listing")
            return []

    def delete_dataset(self, repo_name: str) -> bool:
        """
        Delete a dataset.

        Args:
            repo_name: Name of the dataset (with or without namespace).

        Returns:
            True if deletion was successful, False otherwise.
        """
        # If repo_name doesn't include namespace, add it
        if "/" not in repo_name:
            repo_id = f"{self.username}/{repo_name}" if self.username else repo_name
        else:
            repo_id = repo_name

        logger.info(f"Deleting dataset: {repo_id}")

        api = self._client.api
        if api is not None and HF_HUB_AVAILABLE:
            try:
                api.delete_repo(repo_id=repo_id, repo_type="dataset")
                logger.info(f"Dataset deleted: {repo_id}")
                return True

            except Exception as e:
                logger.error(f"Failed to delete dataset: {e}")
                return False
        else:
            # Simulation mode
            logger.info(f"Simulating dataset deletion for {repo_id}")
            return True

    def upload_traces(
        self,
        traces: List[Dict[str, Any]],
        repo_name: str,
        config: Optional[DatasetConfig] = None,
    ) -> str:
        """
        Upload raw traces as a dataset.

        Converts traces to JSONL format and uploads them.

        Args:
            traces: List of trace dictionaries.
            repo_name: Name for the dataset repo.
            config: Dataset configuration.

        Returns:
            URL of the created dataset.
        """
        import tempfile

        # Create temporary directory with traces
        with tempfile.TemporaryDirectory() as tmpdir:
            tmppath = Path(tmpdir)

            # Write traces as JSONL
            train_file = tmppath / "train.jsonl"
            with open(train_file, "w", encoding="utf-8") as f:
                for trace in traces:
                    f.write(json.dumps(trace) + "\n")

            # Upload
            return self.upload_training_data(
                local_path=tmppath,
                repo_name=repo_name,
                config=config,
                metadata={"type": "traces", "count": len(traces)},
            )

    def __repr__(self) -> str:
        """String representation."""
        return f"<HFDatasetManager user={self.username}>"
