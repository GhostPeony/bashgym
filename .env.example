# =============================================================================
# Project Ouroboros - Environment Variables
# =============================================================================
# Copy this file to .env and fill in your values:
#   cp .env.example .env
# =============================================================================

# -----------------------------------------------------------------------------
# API Keys (Required for full functionality)
# -----------------------------------------------------------------------------

# Anthropic API key for Claude (Teacher Agent)
# Get yours at: https://console.anthropic.com/
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# NVIDIA API key for NeMo services (Optional)
# Get yours at: https://build.nvidia.com/
NVIDIA_API_KEY=your-nvidia-api-key-here

# Hugging Face token for model access (Optional)
# Get yours at: https://huggingface.co/settings/tokens
HF_TOKEN=your-huggingface-token-here

# -----------------------------------------------------------------------------
# HuggingFace Pro Integration (Optional - for Pro features)
# -----------------------------------------------------------------------------
# Pro subscription unlocks: Jobs, ZeroGPU Spaces, 1TB storage, Inference credits
# Subscribe at: https://huggingface.co/subscribe/pro

# HuggingFace username (for namespace resolution)
HF_USERNAME=your-hf-username

# Organization to bill to (optional - for team usage)
HF_ORG=

# Enable Pro features (auto-detected from token, but can be overridden)
HF_PRO_ENABLED=

# Default repo for dataset storage
HF_STORAGE_REPO=

# Default repo for trained models
HF_MODELS_REPO=

# Inference provider: serverless, dedicated, together, replicate, sambanova
HF_INFERENCE_PROVIDER=serverless

# Inference routing strategy: cheapest, fastest, quality
HF_INFERENCE_ROUTING=cheapest

# Default hardware for Jobs: t4-small, a10g-small, a10g-large, a100-large
HF_DEFAULT_HARDWARE=t4-small

# Default timeout for Jobs (minutes)
HF_JOB_TIMEOUT_MINUTES=60

# -----------------------------------------------------------------------------
# NeMo Configuration (Optional - for cloud training)
# -----------------------------------------------------------------------------

# NeMo Data Designer endpoint
NEMO_ENDPOINT=http://localhost:8000

# NVIDIA Inference Microservices endpoint
NIM_ENDPOINT=https://integrate.api.nvidia.com/v1

# NIM model for data augmentation
# Available models: qwen/qwen2.5-coder-32b-instruct, qwen/qwen2.5-coder-7b-instruct
NIM_MODEL=qwen/qwen2.5-coder-32b-instruct

# -----------------------------------------------------------------------------
# Data Augmentation Settings
# -----------------------------------------------------------------------------

# Augmentation provider: anthropic (higher quality) or nim (cost-effective)
AUGMENTATION_PROVIDER=anthropic

# Anthropic model for augmentation (if using anthropic provider)
# Claude 4.5: claude-sonnet-4-5-20250929, claude-haiku-4-5-20251001, claude-opus-4-5-20251101
# Claude 4 (legacy): claude-sonnet-4-20250514
ANTHROPIC_AUGMENTATION_MODEL=claude-sonnet-4-5-20250929

# -----------------------------------------------------------------------------
# Training Settings
# -----------------------------------------------------------------------------

# Base model for fine-tuning (this is YOUR model that gets trained)
BASE_MODEL=Qwen/Qwen2.5-Coder-1.5B-Instruct

# Output directory for trained models
OUTPUT_DIR=data/models

# Learning rate (default: 2e-5)
LEARNING_RATE=2e-5

# Batch size for training
BATCH_SIZE=4

# Number of training epochs
NUM_EPOCHS=3

# Use NeMo Gym for cloud training (default: false = local training)
USE_NEMO_GYM=false

# Use Flash Attention 2 (requires compatible GPU)
USE_FLASH_ATTENTION=true

# -----------------------------------------------------------------------------
# Docker Settings (for sandboxed execution)
# -----------------------------------------------------------------------------

# Docker daemon socket
# DOCKER_HOST=unix:///var/run/docker.sock      # Linux/Mac
# DOCKER_HOST=npipe:////./pipe/docker_engine   # Windows with Docker Desktop

# Docker image for sandboxes
SANDBOX_IMAGE=python:3.10-slim

# Memory limit for sandbox containers
SANDBOX_MEMORY=2g

# CPU limit for sandbox containers
SANDBOX_CPU=2.0

# Network mode for sandboxes (none = isolated, bridge = internet access)
SANDBOX_NETWORK=none

# Base directory for workspaces
WORKSPACE_BASE=/tmp/ouroboros_workspaces

# -----------------------------------------------------------------------------
# Verification Settings
# -----------------------------------------------------------------------------

# Timeout for verification tests (seconds)
VERIFY_TIMEOUT=300

# Maximum retries for flaky tests
VERIFY_MAX_RETRIES=1

# -----------------------------------------------------------------------------
# Routing Settings (Model selection)
# -----------------------------------------------------------------------------

# Routing strategy: teacher_only, student_only, confidence_based, progressive
ROUTING_STRATEGY=confidence_based

# Confidence threshold for using student model
CONFIDENCE_THRESHOLD=0.7

# Maximum student model usage rate
MAX_STUDENT_RATE=0.9

# -----------------------------------------------------------------------------
# Logging Settings
# -----------------------------------------------------------------------------

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Log file (leave empty for stdout only)
LOG_FILE=

# Enable Prometheus metrics
ENABLE_METRICS=false

# Metrics port
METRICS_PORT=9090
